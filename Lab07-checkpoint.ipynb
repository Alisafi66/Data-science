{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myxw88CioHr7"
   },
   "source": [
    "# **Lab 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwudenxXn8gT"
   },
   "source": [
    "## Data Mining: Clustering & Association Rule Mining\n",
    "\n",
    "In this lab, you will learn how to use clustering to find data that are similar to each other based on data attributes, without the need of having any labels to identify them. You will also use scatter plots to visualize these groups more clearly and meaningfully. You will also learn how to perform market basket analysis using associative rule mining. In particular, we will focus on the popular *apriori algorithm*, and also revise on the concepts of support, confidence and lift, looking into how these metrics can be used in the creation of rules that will mine for common patterns of items and other useful insights such as likely product pairings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M0erAcTLn8gU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w61B4Zd1n8gY"
   },
   "source": [
    "Let's use sample data from a popular Kaggle dataset on [World Happiness Report](https://www.kaggle.com/unsdsn/world-happiness). The 2017 data, which we will be using, contains data from 155 countries, with 12 attributes:\n",
    "* Country\n",
    "* Happiness Rank\n",
    "* Happiness Score\n",
    "* High Whisker (of the Happiness Score)\n",
    "* Low Whisker (of the Happiness Score)\n",
    "* Economy (GDP per capita)\n",
    "* Family\n",
    "* Health (Life Expectancy)\n",
    "* Freedom\n",
    "* Trust (Government Corruption)\n",
    "* Generosity\n",
    "* Dystopia Residual\n",
    "\n",
    "To know more about these attributes, read the description of the columns [here](https://www.kaggle.com/unsdsn/world-happiness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_Vqd-hen8gZ"
   },
   "outputs": [],
   "source": [
    "wh = pd.read_csv(\"whr_2017.csv\") #Read the dataset\n",
    "wh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT6PU6xrn8gc"
   },
   "outputs": [],
   "source": [
    "wh.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgOYFTaBn8gg"
   },
   "source": [
    "You can use `shape` property on a DataFrame as well. It will get you the actual \"size\" of the DataFrame (rows x columns)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQTdWV5Yn8gg"
   },
   "outputs": [],
   "source": [
    "wh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X3CjDcFn8gj"
   },
   "source": [
    "To check the datatypes of each attribute (column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5l1us7Jn8gk"
   },
   "outputs": [],
   "source": [
    "wh.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKyjnB9Un8gn"
   },
   "source": [
    "Look at Country. Since string data types have variable length, it is stored in a DataFrame as `object` type. If you extract out the actual value from the Country column, we could see that it is a `str` type after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2MKf0gmn8gn"
   },
   "outputs": [],
   "source": [
    "s = wh['Country'][0]\n",
    "print(s)\n",
    "print(type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fezghrjFn8gq"
   },
   "source": [
    "### Correlational Analysis\n",
    "\n",
    "Typically, when given some data, and assuming that it's almost clean and error-free, you should do a quick correlational analysis to examine the relationship between the numerical attributes. \n",
    "\n",
    "Let's take a subset of the attributes. Due to some redundancies, some attributes are definitely correlated to each other. For instance, Happiness Rank, Whisker.high and Whisker.low are all correlated to Happiness.Score. We only need one happiness measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SOhZ97ORn8gr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wh1 = wh[['Happiness.Score','Economy..GDP.per.Capita.','Family','Health..Life.Expectancy.', 'Freedom', \n",
    "            'Generosity','Trust..Government.Corruption.','Dystopia.Residual']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPx2XhoOn8gv"
   },
   "outputs": [],
   "source": [
    "cor = wh1.corr()\n",
    "display(cor)    # this function only works in notebooks\n",
    "                # it shows everything in dataframe view, compared to standard print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjFX749cn8gx"
   },
   "source": [
    "**Seaborn** package has a nice heatmap visualization function that pretty much handles the coloring, shading and labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MsD9kzJ2n8gy"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_tcS629n8g0"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cor, square = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0jlTQi1n8g3"
   },
   "source": [
    "**Q1**: What can you observe from this correlation heatmap?\n",
    "\n",
    "**Answer**: *type your answer here by double-clicking this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us3abcOun8g3"
   },
   "source": [
    "### Clustering of Countries\n",
    "\n",
    "Since clustering is sensitive to the range of data, it is important that we **scale** the data before clustering. We now use [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from scikit-learn package to standardize the attribute data. Standardization involves removing the mean followed by scaling to unit variance (i.e. dividing by the standard deviation of the data distribution). It is sometimes also known as **z-normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "haFgn2cTn8g4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # For scaling dataset\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BO44Vcbln8g7"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(wh1)\n",
    "print(wh1)\n",
    "print(X)     # observe the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jesMZYnn8g9"
   },
   "source": [
    "`scikit-learn` is a rich machine learning library that provides quite a number of popular [clustering]((https://scikit-learn.org/stable/modules/clustering.html)) algorithms: [**k-means clustering**](https://scikit-learn.org/stable/modules/clustering.html#k-means), [**agglomerative clustering**](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), [**affinity propagation**](https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation) and many more.\n",
    "\n",
    "Let's try with k-means. The first parameter defines the number of cluster that you wish to find. The `verbose` parameter just controls the amount of output the function prints out for checking purposes (0 for no output and a larger integer for more output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlS7P4Mvn8g_"
   },
   "outputs": [],
   "source": [
    "model = KMeans(2, verbose=0)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBGCLIXvn8hC"
   },
   "source": [
    "The model of this k-means function contains the following four properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvS1uW3bn8hD"
   },
   "outputs": [],
   "source": [
    "print(model.cluster_centers_.shape)\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJdh51azn8hG"
   },
   "outputs": [],
   "source": [
    "print(model.labels_.shape)\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ2E1ei6t2Zt"
   },
   "outputs": [],
   "source": [
    "wh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLJNnXmUn8hI"
   },
   "outputs": [],
   "source": [
    "model.inertia_ \n",
    "#A good model is one with low inertia AND a low number of clusters (K). However, this is a tradeoff because as K increases, inertia decreases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0kS5QvRn8hL"
   },
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs_eI0Z6n8hN"
   },
   "source": [
    "Here's some quick explanation of these properties:\n",
    "* `cluster_centers_` contains the centers of the two clusters found thru k-means algorithm. Each of these centers have 8 values. **Note**: Does it make sense? \n",
    "* `labels_` contains the cluster labels for each data point. Since we set k=2, we get either labels 0 or 1 assigned to the data points. Since k-means clustering is an unsupervised learning method, this labels are NOT to be confused with labels that we fixed for the purpose of supervised learning. These are merely labels to indicate the data points' membership to the clusters. \n",
    "* `inertia_` is actually the sum of squared distances (SSD) of samples to their cluster centers. **Note**: Recall what is this SSD?\n",
    "\\begin{equation}\n",
    "    SSD = \\sum_{j=1}^{k}\\sum_{i=1}^{n}\\lVert x_i^{(j)} - c_j\\rVert\n",
    "\\end{equation}\n",
    "* `n_iter_` is the number of iterations k-means took to arrive at stable cluster centers.\n",
    "\n",
    "Let's do a simple label check to see how k-means has done its job. \\\n",
    "\n",
    "We know that the original data has already been sorted by happiness rank. If you didn't notice this earlier, take a look now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD41diBBn8hO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wh.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3C2hJ3En8hR"
   },
   "source": [
    "The labels that were returned are also following the same sequence as the original data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb9U0Mi_n8hS"
   },
   "outputs": [],
   "source": [
    "kmeans_labels = pd.DataFrame(model.labels_)   # put into a DataFrame. We will use this shortly...\n",
    "kmeans_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECLbTrQBn8hZ"
   },
   "source": [
    "Check again the entire list of labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjemOFA_n8hZ"
   },
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HATOgrrn8hd"
   },
   "source": [
    "It appears that k-means has sort of grouped them into two clusters: One cluster (0 or 1) for countries that have a high happiness rank, and the other cluster (with the other number) for countries that have a low happiness rank. Looks like the clustering have done a decent job, except that there seems to be some ambiguity among the countries that are ranked near the middle. \n",
    "\n",
    "Now let us try to visualize these clusters in a more intuitive manner. It would be good to colour the data points with a similar colour according to the clusters they belong, and see where they lie in a certain 2D or 3D space. We have 8 attributes used for clustering. We cannot show all 8, so will have to do some picking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "YPzn-qaCn8hd"
   },
   "outputs": [],
   "source": [
    "# First, add the labels DataFrame as a new column of the wh1 DataFrame. This will enable us to plot more easily.\n",
    "wh1.insert((wh1.shape[1]), 'kmeans', kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU4_O5OTn8hh"
   },
   "outputs": [],
   "source": [
    "wh1.loc[:, 'Family':]     # take a peek at the back columns. Look at the last column\n",
    "wh1.loc[:, :]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEXgS8nYn8hj"
   },
   "outputs": [],
   "source": [
    "# Plot a scatter plot, since we want to see where the values of 2 attributes lie\n",
    "v1 = wh1['Economy..GDP.per.Capita.']\n",
    "v2 = wh1['Health..Life.Expectancy.']\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(v1, v2, c=kmeans_labels[0],s=50,cmap='jet',alpha=0.70)\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel(v1.name)\n",
    "ax.set_ylabel(v2.name)\n",
    "plt.colorbar(scatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ_1e3E_n8hn"
   },
   "source": [
    "**Q2**: Convert the snippet of code in the cell before this, into a function that takes in two Series containing the two columns that you wish to plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qU3yv_Ven8hn"
   },
   "outputs": [],
   "source": [
    "# write function 'plot_kmeans_scatter'\n",
    "v1 = wh1['Dystopia.Residual']\n",
    "v2 = wh1['Trust..Government.Corruption.']\n",
    "\n",
    "plot_kmeans_scatter(v1, v2)\n",
    "\n",
    "# try to visualize the scatter between two attributes that are not so highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRnFDzCyn8hs"
   },
   "source": [
    "**Q3**: Repeat all relevant steps to perform k-means clustering with different number of clusters (k=3, k=4, etc.). Observe again what happens to the labels that have been assigned, either by looking at the order of labels, or by visualizing it on the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYvr8cdXn8ht"
   },
   "outputs": [],
   "source": [
    "# grab the relevant codes from the above cells to re-run k-means on different k. You could \"upgrade\" your\n",
    "# scatter plot function created in Q2 to take in k value; hence generating different number of groups\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooEyZqcfn8hv"
   },
   "source": [
    "## Market Basket Analysis with Associative Rule Mining\n",
    "\n",
    "We are going to leverage on another library (most likely you don't have it yet) called **mlxtend** created by Sebastian Raschka (who wrote a popular hands-on [Python machine learning book](https://github.com/rasbt/python-machine-learning-book)), which contains machine learning extensions to Python's standard scientific computing packages. It overlaps with many parts of **scikit-learn**, but offers some really useful functionalities for association rule mining. To install `mlxtend`, you really don't need to exit the notebook. \n",
    "> If you are on Colab, it should be already installed.<br>\n",
    "If you are using Anaconda, just open another Anaconda prompt, and install it into your environment. <br>`pip install mlxtend`\n",
    "\n",
    "The following lines of code should work if the package is installed in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "W86b8rp7n8hv"
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4LPimXNn8hz"
   },
   "source": [
    "We shall use a relatively large [online retail dataset](http://archive.ics.uci.edu/ml/datasets/online+retail) from the famous UCI ML repository. You do not need to download the data file, it is already available in your lab zip file. For Colab, upload the dataset file to the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "nND2dc4Mn8h0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('online_retail.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUTT1Oosn8h2"
   },
   "source": [
    "There's more than 541K rows of transactional data. Phew!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PohiYWTcn8h2"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYTjnbYCn8h4"
   },
   "source": [
    "### Clean it\n",
    "\n",
    "First thing in your mind: clean it first, or at least find out if anything can be standardized or tidied up for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "i9AMpXhrn8h5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean up spaces in description\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "\n",
    "# Remove any rows that don't have an invoice value. Maybe these transactions are not valid?\n",
    "df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n",
    "\n",
    "# Convert invoice value to strings since we are unlikely to use these values numerical. Make them strings to be safe.\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n",
    "\n",
    "# Remove the credit transactions (those with invoice numbers containing C).\n",
    "df = df[~df['InvoiceNo'].str.contains('C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4NHhVt9n8h7"
   },
   "source": [
    "Check again. We now have about 9K entries removed due to problems in their invoice numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_qfHwcBn8h8"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDHdPQ0In8h_"
   },
   "source": [
    "In order to experiment with a small segment of customers, let's look at sales from France alone. What we are interested is a summarized table showing the data entries on the rows and the items on the columns. Group the data by 'InvoiceNo' then 'Description', the value used should be 'Quantity'. Then, set all the null (NaN) values to 0's (those with values will not be affected). Above all, set the the index (from the original row number) to 'InvoiceNo'.\n",
    "\n",
    "This whole process consolidates the items into 1 transaction per row with each product (column side) showing its quantity sold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfDx-TYMn8h_"
   },
   "outputs": [],
   "source": [
    "basket = (df[df['Country'] ==\"France\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo')\n",
    "         )\n",
    "basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGXWb-jOn8iE"
   },
   "source": [
    "Here's a selected section of the extracted data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueqtLMrQn8iE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show a subset of columns\n",
    "basket.iloc[:,[601,602,603,604,605,606,607,608]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNzdO6_2n8iG"
   },
   "source": [
    "To look at the what was bought in the first invoice... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpzhUa-mn8iG"
   },
   "outputs": [],
   "source": [
    "f = basket.iloc[0,:]    # grab the first invoice, at row 0\n",
    "f[f!=0]                 # just a quick way to show all the nonzero entries in the first invoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLIolNeQn8iI"
   },
   "source": [
    "We do not need the actual quantities. Just need to know if the product was purchased or not purchased. So, we can convert the data in each row to one-hot encoded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "MvIU750Bn8iJ"
   },
   "outputs": [],
   "source": [
    "# Convert the units to 1 hot encoded values\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "# apply the function to the dataframe\n",
    "basket_sets = basket.applymap(encode_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xqREMSmn8iO"
   },
   "outputs": [],
   "source": [
    "basket_sets.iloc[:,[601,602,603,604,605,606,607,608]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUzCbjDkn8iU"
   },
   "source": [
    "> **Side note**: `mlxtend` provides an easy function to perform this conversion if you had started off with transaction data that contains a list of items sold. For instance, if you had data looking like this (each row is a transaction of items sold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "6tkD3QDVn8iU"
   },
   "outputs": [],
   "source": [
    "minidata = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD8LOVDIn8iZ"
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(minidata).transform(minidata)\n",
    "basket_sets2 = pd.DataFrame(te_ary*1, columns=te.columns_)   # simple hack: multiply boolean with 1 converts it to number\n",
    "basket_sets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYmQUkYun8ic"
   },
   "source": [
    "Back to the one-hot encoded basket set, we now drop the 'POSTAGE' column as it is of no use. It's probably a service that the retail shop provided for sending the items, not a product that we want to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "aNXwD6O_n8id"
   },
   "outputs": [],
   "source": [
    "# No need to track postage\n",
    "basket_sets.drop('POSTAGE', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjHrUEZwn8ik"
   },
   "source": [
    "### Support\n",
    "\n",
    "Now that the data has been properly prepared, we can now generate frequent item sets that have a **support** of at least 7% (this number was chosen to get enough useful examples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDYMFCdhn8il"
   },
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsU7VIT4n8in"
   },
   "source": [
    "What is the meaning of **support**?\n",
    "\n",
    "**Support** is one of the three commonly used metrics in association rule mining. It determines how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\text{Support} = \\frac{frequency(itemset)}{N}\n",
    "\\end{equation}\n",
    "\n",
    "Let's calculate this manually to prove that it is correct. First, find all nonzero entries in a particular column, i.e. 'ALARM CLOCK BAKELIKE PINK'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io_HDqaQn8io"
   },
   "outputs": [],
   "source": [
    "alarmclock = basket.loc[:,'ALARM CLOCK BAKELIKE PINK'].to_numpy().nonzero()\n",
    "alarmclock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quWbCjGLn8ir"
   },
   "source": [
    "The support for this item (the earlier example shows support=0.102041, see third row) is the number of nonzero entries divided by the total number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYYQ39Sin8is"
   },
   "outputs": [],
   "source": [
    "print(alarmclock[0].size)\n",
    "print(len(basket_sets))\n",
    "alarmclock[0].size/len(basket_sets)   # double check with the entry in the 3rd row earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJND8o0bn8iv"
   },
   "source": [
    "### Confidence\n",
    "\n",
    "The second measure, **Confidence**, expresses how likely item Y is purchased when item X is purchased, expressed as {$X \\rightarrow Y$}. This is measured by the proportion of transactions with item X, in which item Y also appears. This metric is useful to identify pairs of products that are often sold (or bought by the customer) together. The higher the Confidence score, the more likely items X and Y are found together. A confidence score of 1 (perfect) indicates that X and Y are always bought together.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\text{Confidence} = \\frac{frequency(X,Y)}{frequency(X)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRJFRexLn8iw"
   },
   "outputs": [],
   "source": [
    "rules1 = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "rules1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zB2-nCrrn8i1"
   },
   "source": [
    "> **Side Observation**: Rules are generally in the format of $X\\rightarrow Y$ (meaning X \"implies\" Y, or rather, when item X is purchased, Y is purchased as well). The X is the antecedent, while Y is the consequent. Notice that there can be multiple antecedents and consequents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxiun9Eln8i2"
   },
   "outputs": [],
   "source": [
    "rules1.loc[:,'antecedents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbKUgpSOn8i4"
   },
   "outputs": [],
   "source": [
    "rules1.iloc[14]['antecedents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItmsAXhTn8i7"
   },
   "outputs": [],
   "source": [
    "rules1.iloc[14]['consequents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcx3JSLyn8jH"
   },
   "source": [
    "You can limit the length of each itemset using `max_len` parameter in the [`apriori()`](http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBz7kGQ8n8jI"
   },
   "outputs": [],
   "source": [
    "# setting max_len=2 means that each itemset (group of items) contains a maximum of 2 items, basically 1 antecedent, 1 consequent\n",
    "frequent_itemsets_2 = apriori(basket_sets, min_support=0.07, use_colnames=True, max_len=2)\n",
    "rules1b = association_rules(frequent_itemsets_2, metric=\"confidence\", min_threshold=0.7)\n",
    "print(rules1b)\n",
    "print(rules1b.loc[:,'antecedents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiQM0yIjn8jX"
   },
   "source": [
    "Now, to get back on track, let's calculate this manually to ensure we see for ourselves how Confidence is measured.\n",
    "\n",
    "**Q4**: Calculate the Confidence of a customer buying 'ALARM CLOCK BAKELIKE PINK' also buying 'ALARM CLOCK BAKELIKE GREEN' (answer earlier is 0.725, so you should get this value after calculating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PKOIR5Un8jY"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv57QWQtn8jc"
   },
   "source": [
    "### Lift\n",
    "\n",
    "The third measure **Lift** indicates how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. The popularity control helps to alleviate the inflation of Confidence score caused by an item being more popular than other items. This is calculated by the support of X and Y (seen together) divided by the product of the supports of X and Y individually.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\text{Lift} = \\frac{Support(X,Y)}{Support(X) * Support(Y)}\n",
    "\\end{equation}\n",
    "\n",
    "A lift value **greater than 1** means that item Y is likely to be bought if item X is bought, while a value **less than 1** means that item Y is unlikely to be bought if item X is bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lz9nHKtGn8jd"
   },
   "outputs": [],
   "source": [
    "# Create the rules\n",
    "rules2 = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BM1IkOwSn8jg"
   },
   "source": [
    "So, there are altogether 25 pairings of products that are likely to be bought together by customers from France. \n",
    "\n",
    "**Actionable insight #1**: This mined information can be used for targeted advertisements or to determine which items can be shown together on the online e-commerce portal to maximize potential sales. Promotional discounts can also be given to the purchase of these items together.\n",
    "\n",
    "Again, let's calculate to ensure we are able to obtain the Lift score correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZH7dxK2n8jh"
   },
   "outputs": [],
   "source": [
    "alarm_pink = basket.loc[:,'ALARM CLOCK BAKELIKE PINK'].to_numpy().nonzero()\n",
    "alarm_green = basket.loc[:,'ALARM CLOCK BAKELIKE GREEN'].to_numpy().nonzero()\n",
    "alarm_pink_and_green = list(set(alarm_pink[0]) & set(alarm_green[0]))\n",
    "print(len(alarm_pink_and_green))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTfY6A4qn8jj"
   },
   "source": [
    "**Q5**: The number of transactions where the pink and green alarm clocks have been purchased together has been calculated above. Calculate the Lift score. (answer should be 7.4789 as found earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obAbVgtKn8jk"
   },
   "outputs": [],
   "source": [
    "# complete the following lines\n",
    "supportXY = \n",
    "supportX = \n",
    "supportY = \n",
    "lift = \n",
    "\n",
    "print(supportXY)\n",
    "print(supportX)\n",
    "print(supportY)\n",
    "print(lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvdPgf93n8jo"
   },
   "source": [
    "### Building rules\n",
    "\n",
    "After building the frequent itemset, we can then build rules to figure out what it is telling us. \n",
    "\n",
    "E.g. the most common strategy is to construct a few rules with a high lift value, which means that it occurs more frequently than would be expected (since this considers the item popularity) given the number of transaction and product combinations. We can also build rules based on high confidence score as well, if you are not concerned so much with lift. This part of the analysis is where the *domain knowledge* will come in handy. As a data scientist, we have to work closely with domain experts or relevant stakeholders to construct meaningful rules.\n",
    "\n",
    "Let's filter the data based on a large lift (6) and high confidence (0.8). We find products (or rather pairs of products) that are above these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBI7cZw8n8jp"
   },
   "outputs": [],
   "source": [
    "rules2[ (rules2['lift'] >= 6) &\n",
    "        (rules2['confidence'] >= 0.8) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qEsMgH-n8jr"
   },
   "source": [
    "It appears that the green and red alarm clocks are purchased together and the red paper cups, napkins and plates are purchased together more frequently than the overall levels of probability (see Support values) would suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1noDIHsn8jr"
   },
   "outputs": [],
   "source": [
    "print(basket['ALARM CLOCK BAKELIKE GREEN'].sum())\n",
    "print(basket['ALARM CLOCK BAKELIKE RED'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjYj_OB9n8jt"
   },
   "source": [
    "**Actionable insight #2**: At this point, you may want to look at how much opportunity there is to use the popularity of one product to drive sales of another. Here, we can see that we sold 340 Green Alarm clocks but only 316 Red Alarm Clocks so maybe we can drive more Red Alarm Clock sales by recommending it to those who have bought Green Alarm clocks.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
