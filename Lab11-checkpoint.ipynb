{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWgMJRgN4Tm8"
   },
   "source": [
    "# **Lab 11: PySpark Dataframes and Spark SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHT7Ff94YDyq"
   },
   "source": [
    "# [Pyspark Dataframes](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html)\n",
    "\n",
    "A distributed collection of data grouped into named columns.A DataFrame is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession. Once created, it can be manipulated using the various domain-specific-language (DSL) functions.  \n",
    "\n",
    "Credit goes to [website](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/) for examples of Pyspark dataframe operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZmMrV3T7NJn"
   },
   "source": [
    "### So, is there any differences between Lab10's Pyspark RDD and Pyspark DataFrames?  \n",
    "From an [article](https://sachee.medium.com/apache-spark-dataframe-vs-rdd-24a04d2eb1b9):\n",
    "\n",
    "*   Pyspark DataFrames require a schema and you can think of them as “tables” of data. RDDs are less structured and closer to Scala collections or lists.\n",
    "*   Operations on Pyspark DataFrames are optimizable by Spark whereas operations on RDDs are imperative and run through the transformations and actions in order.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrbthzan8pQu"
   },
   "source": [
    "## Spark dataframes are immutable\n",
    "When a Spark dataframe is created,the content of the dataframe cannot be changed but a new dataframe will be created. Internally in Spark, commands such as filter() will not change the dataframe but it creates a new dataframe from the output of executing the command. \n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bUp8aRS4x3b"
   },
   "source": [
    "### What about the dataframe that we used in previous labs ?\n",
    "In previous labs, we did run dataframe but remember it is Python's dataframe where we imported the pandas library. It is not Pyspark dataframe.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6kqR85MYDKF"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77cttsKaVaVl"
   },
   "source": [
    "Download Java and Apache Spark with Hadoop. There are many releases for Apache Spark. If you are running in non-Colab environment, find the release that suits your machine. Here is the link to Apache Spark [releases](https://spark.apache.org/downloads.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OSXgEfMoJbHp"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_5efWgbWpz6"
   },
   "source": [
    "Now, it’s time to set the ‘environment’ path.Then we need to install and import the ‘findspark’ library that will locate Spark on the system and import it as a regular library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SyIJbfYLWmRH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
    "!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOSFv2SZW1dB"
   },
   "source": [
    "Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark. Ok, we are done with PySpark setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TtQA_Ry9Wa04"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() #local[3] or local[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cUuYM-gxDIuM"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"IMDB\").setMaster(\"local[*]\")\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXARq5-zDyWA"
   },
   "source": [
    "Let's read the data that we have used in Lab03. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rArdVkfHAp3z"
   },
   "outputs": [],
   "source": [
    "imdb_df = spark.read.csv(\"IMDB_top250.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "766mJDMyJK1X"
   },
   "source": [
    "Let's try to understand the data. First, we need to know the column names and types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUBMjjUkJjIC"
   },
   "outputs": [],
   "source": [
    "imdb_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mllxMPzFJrg1"
   },
   "outputs": [],
   "source": [
    "imdb_df.show(5)  # What is the function used to retrieve the first 5 rows of a Python's data frame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Hb6VwUhKPCG"
   },
   "source": [
    "Let's calculate the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LQKBe73KSBh"
   },
   "outputs": [],
   "source": [
    "imdb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWvvbskhH_V_"
   },
   "outputs": [],
   "source": [
    "imdb_df.show(truncate=False)\n",
    "#What happens when truncate = True ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IVH5is-LZr2"
   },
   "source": [
    "# Let's use Spark's SQL to perform operations on PySpark's dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5aqbLhU-S7S"
   },
   "source": [
    "# Queries: select and Like functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09UwVMGILxam"
   },
   "outputs": [],
   "source": [
    "imdb_df.select(\"Title\",\"Year\",\"Rated\").show(5)  # view the first five rows based on selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCxLsuQm8Rjj"
   },
   "source": [
    "In the brackets of the “Like” function, the % character is used to filter out all titles having the “ THE ” word. If the condition we are looking for is the exact match, then no % character shall be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo78tjyn7eSb"
   },
   "outputs": [],
   "source": [
    "imdb_df.select( \"Title\",\n",
    "imdb_df.Title.like(\"%Godfather%\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzf4vdPw-1Az"
   },
   "source": [
    "# Inspection: distinct() and show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yc3ad8oeNpjg"
   },
   "outputs": [],
   "source": [
    "imdb_df.select(\"Rated\").distinct().show()\n",
    "#The distinct() will come in handy when you want to determine the unique values in the categorical columns in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqTjV4r5N7AH"
   },
   "source": [
    "# Multiple View: groupBy function \n",
    "\n",
    "Can we know how many rows of data associated with each rating category ? We can use the groupBy function to group the dataframe column values and then apply an aggregate function on them to derive some useful insight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ieHstL0OOQB"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F #you can create alias\n",
    "imdb_df.groupBy(\"Rated\").agg(F.count(\"Rated\").alias(\"Total-Row\")).sort(F.desc(\"Total-Row\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDsGFJ3iAE5c"
   },
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRraDg4WMHLv"
   },
   "source": [
    "Often when we are working with numeric features, we want to have a look at the statistics regarding the dataframe. The describe() function is best suited for such purposes.It is pretty similar to Panda’s describe function but the statistical values are far less and the string columns are described as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHYvtn7BMflN"
   },
   "outputs": [],
   "source": [
    "imdb_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hSQrkj-Ombw"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Can data cleaning operations be applied to Pyspark dataframe ? Yes, of course. Let's look at some examples of data cleaning operations. We will use the Automobile data from Lab04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9SYvrDL2Sx6k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "cols = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "cars = pd.read_csv('imports-85.data.txt', names=cols)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7D9kg9VXzZP"
   },
   "source": [
    "Once we have read the Automobile data, we convert them to Pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d66j4_z3Wat1"
   },
   "outputs": [],
   "source": [
    "cars_df = spark.createDataFrame(cars.astype(str))\n",
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Of9PhJbYf7Q"
   },
   "source": [
    "From the first glance of the above code, there are many \"?\"\"where the data is missing. Let's count the number of \"?\" in each column of the Pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8h9y1u6Ru8O"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cars_df.agg(*[\n",
    "    F.count(F.when(F.col(str(c)) == \"?\",1)).alias(c) for c in cars_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwN4a9uuY3A2"
   },
   "source": [
    "The column *normalized-losses* has the most \"?\". Let's replace the missing values in this column with the most frequently occurred value. The code below helps to identify the frequency of each distinct value in the column *normalized-losses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k5SBiBhllnS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cars_df.groupBy(\"normalized-losses\").agg(F.count(\"normalized-losses\").alias(\"Total\")).sort(F.desc(\"Total\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnkkuVYmZhWX"
   },
   "source": [
    "From the above table, the value 161 is the most frequently occured values in the column normalized-losses. Let's replace the \"?\" with the value 161. The value 161 must be in the form of a string because the entire PySpark dataframe is of type String. Refer to cell[79]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MQUDH77yQ1oQ"
   },
   "outputs": [],
   "source": [
    "cars_df = cars_df.replace('?', '161', subset= \"normalized-losses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnQtma5paFIE"
   },
   "source": [
    "Checkout whether the replacement is done. Looks like replacement is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16eJ7LUiRBfA"
   },
   "outputs": [],
   "source": [
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmIKMtUFaL_h"
   },
   "source": [
    "There are other columns having \"?\" as well. However it is not as many as the column *normalized-losses*. Let's just replace it using a default value which is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btb482MHlkpD"
   },
   "outputs": [],
   "source": [
    "cars_df = cars_df.replace('?', '0')\n",
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PsNvla_aiDj"
   },
   "source": [
    "Let's do a check again. Is there any columns having \"?\" as value ? No, there are no columns having \"?\" as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgdj43MeMmxb"
   },
   "outputs": [],
   "source": [
    "cars_df.agg(*[\n",
    "    F.count(F.when(F.col(str(c)) == \"?\", 1)).alias(c) for c in cars_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHDUYwRKayWT"
   },
   "source": [
    "We can convert the column type too. Suppose that we would like to convert the column *symboling* from String to Integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uqnEFBW2RqQF"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "cars_df = cars_df.withColumn(\"symboling\", F.col(\"symboling\").astype(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaTHR8GSbGcR"
   },
   "source": [
    "Let's double-check whether the conversion is done. Yes, the conversion is successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiN49r7MSa4s"
   },
   "outputs": [],
   "source": [
    "cars_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLitm7rfbV5F"
   },
   "source": [
    "We can create a new column. Let's name the column as *double-symboling*. This new column will multiply the value from the column *symboling* with 2. Use the show() command to check whether the new column and the values have been created / computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqHiGbGOSe8z"
   },
   "outputs": [],
   "source": [
    "cars_df.withColumn(\"double-symboling\", cars_df.symboling*2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr1qn1izcJN3"
   },
   "source": [
    "# Output File\n",
    "\n",
    "There are many ways to export the Pyspark dataframe as csv. As shown in the following cells are two methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDg1_FuecbNG"
   },
   "source": [
    "**Method 1**: Convert it to pandas dataframe. You will see the file appearing in the folder within this Colab session. Once the file is in pandas dataframe, you can use the Python dataframe manipulation operations on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LNrK7oIVQMWj"
   },
   "outputs": [],
   "source": [
    "cars_df.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzVtmpYjc9nr"
   },
   "source": [
    "**Method 2:**Write it out as csv. If you check the folder, there are two csv files. Why are there two csv files ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fsR3a7_pQnfx"
   },
   "outputs": [],
   "source": [
    "cars_df.write.csv('mycsv1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbYSKsgne5Rm"
   },
   "source": [
    "Remember, we are still in Pyspark environment. Let's find out the number of partitions. It turned out that we have two partitions. Remember, Pyspark writes out one file per partitions, so that is why we have to csv file. Pyspark does parallel write operation. We do not have control over the file naming.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzwFVSVHdr1n"
   },
   "outputs": [],
   "source": [
    "cars_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJVWIStJfD-M"
   },
   "source": [
    "Since we want only a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "q-q32808f93A"
   },
   "outputs": [],
   "source": [
    "cars_df.coalesce(1).write.csv(\"/content/mycsv2\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "K3HK19_HIVab"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
